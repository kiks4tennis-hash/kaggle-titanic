## Hypothesis
tune hyperparameters of Decision Tree 
because the process can prevent overfitting.

## Model
Decision Tree

## Parameters
'criterion': 'entropy' 
'max_depth': 5
'min_samples_leaf': 4
'min_samples_split': 2

## Validation Score
Training Accuracy: 0.8624
Validation Accuracy: 0.8324

## Kaggle LB
Score: 0.75837

## Conclusion
- this experiment mitigated overfitting.
- train and val score improved compared with Logistic Regression, but test score didn't.

## Next Experiment
